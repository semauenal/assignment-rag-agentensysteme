{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval-Augmented Generation (RAG)"
      ],
      "metadata": {
        "id": "Qo6lPJ33dHDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2 Implementierung eines KI-Systems mit RAG  und Agentenkomponenten**\n",
        "\n",
        "**2.1 RAG-System**\n",
        "\n",
        "Wie bereits im ersten Teil des Assignments beschrieben, kommen RAG-Systeme (Retrieval-Augmented Generation) immer dann zum Einsatz, wenn externe Wissensquellen in ein Sprachmodell eingebunden werden sollen. Die Motivation liegt darin, dass ein LLM allein nur auf seinem Trainingswissen basiert, während sich durch RAG zusätzlich aktuelle oder spezialisierte Informationen berücksichtigen lassen. Das Vorgehen umfasst im Wesentlichen die Indexierung externer Daten, das semantische Retrieval relevanter Inhalte und die Generierung einer Antwort durch das LLM. Dieses Notebook zeigt exemplarisch, wie ein solches System aufgebaut wird und wie die einzelnen Bausteine zusammenwirken.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "xWNni9nfV5Fy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Auswahl der externen Quelle**\n",
        "\n",
        "Da eines meiner Hobbys das Anschauen von Sport ist, insbesondere **Tennis**, und wir uns derzeit in der Hochsaison befinden, in der gerade Wimbledon zu Ende gegangen ist und nun Cincinnati und die US Open direkt hintereinander folgen, habe ich beschlossen, diese Aufgabe auf diesen Bereich zu konzentrieren. Darüber hinaus hat sich nach der Ära der „Big Three“ eine neue Rivalität im Herrentennis entwickelt, die noch mehr Aufmerksamkeit und Begeisterung für diesen Sport mit sich bringt.  \n",
        "\n",
        "Auf der Suche nach geeigneten Videos oder Dokumenten mit interessanten und relevanten Informationen für diese RAG-Aufgabe bin ich auf ein sehr gutes YouTube-Video gestoßen:  \n",
        "**„Every TENNIS ERA Explained In 10 Minutes”** ([Link](https://www.youtube.com/watch?v=dxg1-JXKG5A)).  \n",
        "\n",
        "Ich habe dieses Video transkribiert und die Transkription in mein GitHub-Repository hochgeladen ([Raw-Link](https://raw.githubusercontent.com/semauenal/assignment-rag-agentensysteme/refs/heads/main/Transcript_Every_Tennis_Era_Explained.txt)). Die Transkription umfasst **1.508 Wörter** und dient als externe Wissensquelle für das in diesem Notizbuch implementierte RAG-System.  \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "FYh8gL8QZ2hT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Erklärungen zum Code**\n",
        "\n",
        "Bevor endgültig der Code zum RAG-System gestartet wird, soll nochmal kurz erklärt werden, auf was Wert gelegt wird beim Implementieren des RAGs.\n",
        "\n",
        "- Kommentare: Es wird versucht, verständliche, aber nicht zu viele Kommentare zu setzen.\n",
        "\n",
        "- Erklären von Abstrahierung und Frameworks bzw. Funktionen: Vorhandene Abstraktionen oder Framework-Aufrufe werden kurz erläutert, damit klar ist, was im Hintergrund passiert.\n",
        "\n",
        "- Immer wieder Print-Ausgaben: Zur Kontrolle werden Zwischenergebnisse ausgegeben, um den Ablauf und die Daten zu prüfen.\n",
        "\n",
        "- Tokennutzung / Limits: Durch Parameter wie max_tokens und die Auswahl weniger Chunks wird der Tokenverbrauch begrenzt und kontrollierbar gehalten.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Q2bWkN-ZnYTw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verbindung zu OpenAI**\n",
        "\n",
        "In diesem Abschnitt wird die Verbindung zur OpenAI-API hergestellt. Dafür wird der API-Schlüssel aus den Secrets geladen.\n",
        "\n",
        "Nutzung der OpenAI-Python-Bibliothek:\n",
        "- Zugriff auf OpenAI-API vereinfacht\n",
        "- Anstatt selbst HTTP-Anfragen an die Endpunkte (z. B. /v1/chat/completions, /v1/embeddings) zu schreiben, bietet die Bibliothek eine fertige Schnittstelle in Python\n",
        "- Klasse OpenAI erstellt einen Client, der mit dem API-Key authentifiziert wird\n",
        "\n",
        "  --> „Brücke“ zwischen dem Notebook und der OpenAI-Cloud dar und übernimmt die ganze low-level Arbeit der API-Kommunikation.\n",
        "\n"
      ],
      "metadata": {
        "id": "qAM2R1UMgy95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# API-Key aus Colab-Secrets laden\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "OPENAI_API_KEY = userdata.get('apikey_sm')\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "Cz0RpUUwXW3z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modellkonfiguration**\n",
        "\n",
        "Damit später bei der Generierung des Outputs nicht ständig erneut die Konfigurationen angegeben werden müssen, werden sie hier einmal zentral definiert.\n"
      ],
      "metadata": {
        "id": "U9ADgpQgodlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Konfiguration durch Variabeln\n",
        "model_chat    = \"gpt-4o-mini\"\n",
        "temperature   = 0               # steuert Kreativität\n",
        "token_limit   = 4096            # internes Budget (Kontrolle, nicht das echte Kontextfenster)\n",
        "\n",
        "top_k         = 4               # Anzahl der Chunks, die beim Retrieval zurückgegeben werden\n"
      ],
      "metadata": {
        "id": "LdDyzXo9odEe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transkript laden\n",
        "\n",
        "# Importieren von requests: Python-Bibliothek für HTTP-Anfragen\n",
        "import requests\n",
        "url = \"https://raw.githubusercontent.com/semauenal/assignment-rag-agentensysteme/refs/heads/main/Transcript_Every_Tennis_Era_Explained.txt\"\n",
        "transcript = requests.get(url).text\n",
        "print(transcript[:500])  # Vorschau"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmPcz58etNok",
        "outputId": "abb45565-231c-4d28-dc08-ef930ae7bcb3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿Back in the day, way before 1968, tennis wasn’t even a competition. It was more about tradition and having fun. The sport lived in black and white. And the players, well, it looked like they were heading to a party and not a professional game. To be fair to them, it wasn’t really a professional sport at the time, but those outfits were still outrageous. I’m talking about the pre-Open Era. This was one of the weirdest eras in the history of this sport. Why? Because this was the time when amateur\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chunking**\n",
        "\n",
        "Beim Aufbau eines RAG-Systems ist **Chunking** wichtig, weil große Dokumente nicht auf einmal in den Prompt passen.  \n",
        "Die Texte werden in Abschnitte zerlegt, damit sie als Embeddings berechnet und später gezielt abgerufen werden können.  \n",
        "Dabei gilt:  \n",
        "- Chunks dürfen nicht zu kurz sein, sonst geht inhaltlicher Kontext verloren.  \n",
        "- Eine **Überlappung** ist sinnvoll, damit am Rand keine relevanten Informationen abgeschnitten werden.  \n",
        "\n",
        "Hier wurde bewusst eine eigene kleine Funktion implementiert, um den Ablauf nachvollziehbar zu machen.  \n",
        "Es gibt zwar viele Frameworks (z. B. in LangChain oder LlamaIndex), die das Chunking mit einer einzigen Zeile erledigen,  \n",
        "aber durch die eigene Funktion wird deutlich, wie der Prozess intern funktioniert.\n",
        "\n",
        "\n",
        "Die Funktion `chunk_text` arbeitet auf Basis einer Schleife:  \n",
        "- Der Text wird zuerst in Wörter aufgesplittet.  \n",
        "- Mit einem Fenster von `chunk_size` Wörtern wird ein Abschnitt gebildet.  \n",
        "- Jeder Abschnitt wird als String gespeichert und in die Liste `chunks` geschrieben.  \n",
        "- Der Startindex verschiebt sich dann jeweils um `chunk_size - overlap`, sodass sich die Abschnitte leicht überlappen.  \n",
        "- Am Ende liefert die Funktion eine Liste von Textabschnitten zurück.  \n",
        "\n"
      ],
      "metadata": {
        "id": "nBcCSQgkvvq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunking\n",
        "# Text in Abschnitte von ca. 500 Wörtern mit Überlappung 30 Wörter\n",
        "def chunk_text(text, chunk_size=300, overlap=30):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(words):\n",
        "        end = start + chunk_size\n",
        "        chunk = \" \".join(words[start:end])\n",
        "        chunks.append(chunk)\n",
        "        start += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_text(transcript)\n",
        "print(\"Anzahl Chunks:\", len(chunks))\n",
        "# Vorschau der ersten 300 Zeichen des ersten Chunks\n",
        "print(\"Beispiel Chunk:\\n\", chunks[0][:300])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsBHm3-wwaBp",
        "outputId": "7d274ca7-006e-4e1b-ee70-e3f320ebf383"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anzahl Chunks: 6\n",
            "Beispiel Chunk:\n",
            " ﻿Back in the day, way before 1968, tennis wasn’t even a competition. It was more about tradition and having fun. The sport lived in black and white. And the players, well, it looked like they were heading to a party and not a professional game. To be fair to them, it wasn’t really a professional spo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embedding**\n",
        "\n",
        "Nach dem Chunking werden die Textabschnitte in sogenannte Embeddings umgewandelt.  \n",
        "Ein Embedding ist ein dichter Vektor, der die semantische Bedeutung eines Textes in einer hochdimensionalen Zahlen­darstellung abbildet. So können Inhalte mathematisch verglichen werden (z. B. über Cosine-Similarity), was für das semantische Retrieval die Grundlage bildet.  \n",
        "\n",
        "Um die Chunks in Embeddings umzuwandeln wird der OpenAI-Client genutzt,  weil er einiges schon übernimmt, was man mit requests selbst machen müsste, wie zum Beispiel Header setzen, JSON bauen und Antworten parsen. Dadurch spart man Code und Fehlerquellen, abstrahiert aber auch nicht zu viel. LangChain wäre zwar noch kürzer, nimmt einem aber viele Details ab und versteckt den Ablauf stärker.\n",
        "\n",
        "\n",
        "Gewähltes Modell  \n",
        "\n",
        "Für die Embedding-Erstellung wird das Modell **`text-embedding-3-small`** von OpenAI genutzt.  \n",
        "- Vorteil: kostengünstig und gleichzeitig qualitativ gut genug für ein mittellanges Transkript.  \n",
        "- Output: pro Textabschnitt ein Vektor mit 1536 Dimensionen.  \n",
        "  - Jede Dimension ist ein Zahlenwert, der eine bestimmte latente Eigenschaft des Textes repräsentiert.  \n",
        "  - Man kann es sich wie einen „Punkt im 1536-dimensionalen Raum“ vorstellen, der die Bedeutung des Textes mathematisch beschreibt.  \n",
        "\n",
        "Abstraktionen im Code  \n",
        "\n",
        "- **`client.embeddings.create(...)`**:  \n",
        "  API-Aufruf an den OpenAI-Endpunkt `/v1/embeddings`.  \n",
        "  Abstrahiert werden: Authentifizierung, Aufbau der Anfrage, Versand und Empfang der JSON-Antwort.  \n",
        "- **`np.array(vector)`**:  \n",
        "  Umwandlung der zurückgegebenen Python-Liste in ein NumPy-Array → effizientere Verarbeitung.  \n",
        "- **`np.vstack(embeddings)`**:  \n",
        "Nimmt die einzelnen Embedding-Vektoren (jeder ist eine Liste mit 1536 Zahlen) und legt sie untereinander in eine Tabelle.\n",
        "Ergebnis: eine 2D-Matrix mit der Form (Anzahl Chunks, 1536).\n",
        "  - Jede Zeile = ein Chunk-Embedding\n",
        "  - Jede Spalte = eine bestimmte Dimension des Vektorraums\n",
        "\n",
        "\n",
        "Zusammenfassung  \n",
        "\n",
        "In diesem Schritt wird also für jeden Chunk ein Embedding erzeugt und zu einer gemeinsamen Matrix zusammengefügt.  \n",
        "Dies ist der zentrale Zwischenschritt, um später Anfragen mit denselben Methoden in denselben Vektorraum einzubetten und semantisch passende Chunks zurückzufinden.\n"
      ],
      "metadata": {
        "id": "F0IvFwbS74P0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Embeddings berechnen\n",
        "embeddings_transcript = []\n",
        "for chunk in chunks:\n",
        "    response = client.embeddings.create(\n",
        "        model=\"text-embedding-3-small\",\n",
        "        input=chunk\n",
        "    )\n",
        "    vector = response.data[0].embedding\n",
        "    embeddings_transcript.append(np.array(vector))\n",
        "\n",
        "embeddings_transcript = np.vstack(embeddings_transcript)\n",
        "\n",
        "# shape von NumPy-Arrays zeigt Form des Arrays an (Dimensionen)\n",
        "print(f\"Anzahl Embeddings: {embeddings_transcript.shape[0]}\")\n",
        "print(f\"Länge pro Embedding-Vektor: {embeddings_transcript.shape[1]} Dimensionen\")\n",
        "# Als Test anzeigen der ersten 10 Dimensionen des ersten Chunks\n",
        "print(embeddings_transcript[0][:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51lL5ov3SLSK",
        "outputId": "7f156970-3660-4ca8-b1de-0a479cba1000"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anzahl Embeddings: 6\n",
            "Länge pro Embedding-Vektor: 1536 Dimensionen\n",
            "[-0.00377981  0.02224585 -0.00795002  0.01334467  0.04593974 -0.00183667\n",
            "  0.00059181 -0.02226005 -0.02113853  0.01954852]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Retrieval**\n",
        "\n",
        "Nach Umwandlung der Chunks in Embeddings, folgt Retrieval-Schritt. Ziel: Aus allen gespeicherten Embeddings die relevantesten Chunks zu einer Anfrage finden.\n",
        "\n",
        "Für diesen Code wurde bewusst ein experimenteller Ansatz gewählt. Statt sofort auf integrierte Lösungen zurückzugreifen, werden hier zwei Hilfsfunktionen selbst definiert. Dadurch wird die Logik nachvollziehbarer.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Cosine-Similarity**\n",
        "\n",
        "Die Cosine-Similarity kommt aus der linearen Algebra und Geometrie.\n",
        "Sie misst den Winkel zwischen zwei Vektoren.\n",
        "Je kleiner der Winkel, desto ähnlicher die Vektoren.\n",
        "- Formal:  \n",
        "\n",
        "$$\n",
        "\\cos(\\theta) = \\frac{a \\cdot b}{||a|| \\cdot ||b||}\n",
        "$$  \n",
        "\n",
        "mit $a \\cdot b$ = Skalarprodukt und $||a||$ = Länge (Norm) des Vektors.  \n",
        "\n",
        "*Wertebereich*\n",
        "- **1** → Vektoren zeigen exakt in dieselbe Richtung → Inhalte sehr ähnlich  \n",
        "- **0** → Vektoren stehen senkrecht (orthogonal) → keine Ähnlichkeit  \n",
        "- **-1** → Vektoren zeigen in entgegengesetzte Richtungen → theoretisch maximale Gegensätzlichkeit (bei Embeddings fast nie relevant)  \n",
        "\n",
        "*Umsetzung im Code*\n",
        "- `a` und `b` = zwei Vektoren (z. B. Query-Embedding und ein Chunk-Embedding)  \n",
        "- `np.dot(a, b)` = Skalarprodukt (Summe der komponentenweisen Multiplikationen)  \n",
        "- `np.linalg.norm(a)` = Länge des Vektors *a*, berechnet als Wurzel der Summe der Quadrate aller Dimensionen  \n",
        "\n",
        "- Eine kleine Konstante `1e-12` wird hinzugefügt, um Division durch Null zu vermeiden.  \n",
        "\n",
        "**Retrieval-Funktion**\n",
        "retrieve_top_k:\n",
        "- Query wird ebenfalls in ein Embedding umgewandelt\n",
        "- Query-Embedding wird mit allen Chunk-Embeddings verglichen\n",
        "- Die besten Treffer (Top-k) werden sortiert und zurückgegeben.\n"
      ],
      "metadata": {
        "id": "E9wUBAZoIVDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cosine Similarity zwischen zwei Vektoren\n",
        "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    return float(np.dot(a, b) / ((np.linalg.norm(a) * np.linalg.norm(b)) + 1e-12))\n",
        "\n",
        "# Retrieval-Funktion: Top-k ähnlichste Chunks\n",
        "def retrieve_top_k(query: str, embeddings: np.ndarray, chunks: list[str], k: int):\n",
        "    # Query → Embedding\n",
        "    embedding_response = client.embeddings.create(model=\"text-embedding-3-small\", input=query)\n",
        "    q_vec = np.array(embedding_response.data[0].embedding, dtype=np.float32)\n",
        "\n",
        "    # Ähnlichkeiten berechnen\n",
        "    sims = [cosine_similarity(q_vec, emb) for emb in embeddings]\n",
        "\n",
        "    # Sortieren & Top-k wählen\n",
        "    top_ids = np.argsort(sims)[::-1][:k]\n",
        "    return [(i, sims[i], chunks[i]) for i in top_ids]\n",
        "\n",
        "\n",
        "# Testaufruf (zur Kontrolle)\n",
        "query_test = \"Who are the Big Three in tennis?\"\n",
        "results = retrieve_top_k(query_test, embeddings_transcript, chunks, top_k)\n",
        "\n",
        "for idx, score, txt in results:\n",
        "    print(f\"Chunk {idx} | Score = {score:.3f}\\n'{txt[:200]}...'\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0ygefZvC_Nl",
        "outputId": "29da6fe4-1c9c-4c19-ca90-ada64ddb3819"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 2 | Score = 0.624\n",
            "'set the tone for what was coming next, as it was the first time the best players were put up against each other for the world to watch. But what came next was something that nobody expected. After the...'\n",
            "\n",
            "Chunk 3 | Score = 0.576\n",
            "'than you’ll ever find anywhere else. But let’s not forget about one other guy who still left his mark in this era despite playing with such greats. Had he not played in the same era as the Big Three, ...'\n",
            "\n",
            "Chunk 4 | Score = 0.515\n",
            "'with a clean game and a cheeky celebration at the end. Alexander Zverev, with his massive serve and lethal forehand, also showed promise. He won an Olympic gold but is still hunting that all-important...'\n",
            "\n",
            "Chunk 1 | Score = 0.498\n",
            "'in 1968. The pros and amateurs were allowed to play together in the same tournaments. And let me say this, things got really spicy really quickly. Money started flowing into the sport. Television came...'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generierung des Outputs**\n",
        "\n",
        "Nun geht es um den letzten Teil der RAG-Pipeline. Es wurde bewusst die Variante über den **OpenAI-Client** gewählt, anstatt Frameworks wie LangChain zu nutzen.  Ziel war es, später mit nur einer Zeile (`ask(...)`) Antworten generieren zu können.  Dabei lag der Fokus besonders auf dem Einbau von Kontrollmechanismen, um sicherzustellen,  dass das Tokenbudget nicht überschritten wird.  \n",
        "\n",
        "\n",
        "Im Folgenden wird der Code etwas detaillierter erklärt:\n",
        "\n",
        "- **Parameter**:  \n",
        "  - `question`: Nutzerfrage  \n",
        "  - `embeddings`, `chunks_in`: vorbereitete Datenbasis aus Embeddings + Textabschnitten  \n",
        "  - `k`: wie viele Top-Chunks beim Retrieval genutzt werden  \n",
        "  - `max_ctx_chars`: begrenzt die Länge des eingebauten Kontexts  \n",
        "  - `max_answer_tokens`: maximale Länge der Modell-Antwort  \n",
        "\n",
        "- **Ablauf**:  \n",
        "  1. Die Nutzerfrage wird eingebettet und mit allen Chunks verglichen → Top-k relevante Chunks.  \n",
        "  2. Der Kontext wird aus diesen Chunks zusammengesetzt, aber auf `max_ctx_chars` Zeichen begrenzt.  \n",
        "  3. Prompt wird erstellt:  \n",
        "     - *System-Message*: definiert Regeln („antworte nur aus dem Kontext“).  \n",
        "     - *User-Message*: enthält Kontext + Nutzerfrage.  \n",
        "  4. Mit `client.chat.completions.create` wird das LLM aufgerufen.  \n",
        "  5. Die erste Modellantwort wird extrahiert und als String zurückgegeben.  \n",
        "\n",
        "Damit ist die komplette RAG-Pipeline abgeschlossen: **eine Nutzerfrage genügt, und das System führt Retrieval, Kontextbau und Antwortgenerierung automatisch aus.*\n"
      ],
      "metadata": {
        "id": "C3iU2QrVe_ov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline-Funktion: Retrieval → LLM\n",
        "def ask(question: str,\n",
        "        embeddings: np.ndarray = embeddings_transcript,\n",
        "        chunks_in: list[str] = chunks,\n",
        "        k: int = top_k,\n",
        "        max_context_chars: int = 3000,\n",
        "        max_answer_tokens: int = 300):\n",
        "\n",
        "    # Top-k Chunks holen\n",
        "    retrieved = retrieve_top_k(question, embeddings, chunks_in, k)\n",
        "\n",
        "    # Kontext zusammenbauen (begrenzt auf max_context_chars)\n",
        "    context_parts, used = [], 0\n",
        "    for idx, score, txt in retrieved:\n",
        "        if used + len(txt) > max_context_chars:\n",
        "            txt = txt[: max_context_chars - used]\n",
        "        context_parts.append(txt.strip())\n",
        "        used += len(txt)\n",
        "        if used >= max_context_chars:\n",
        "            break\n",
        "    context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
        "\n",
        "    # Prompt definieren\n",
        "    system_msg = (\n",
        "        \"You are a concise assistant. Answer ONLY using the provided context. \"\n",
        "        \"If the answer is not in the context, say you don't know.\"\n",
        "    )\n",
        "    user_msg = f\"Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer in 3–6 sentences.\"\n",
        "\n",
        "    # OpenAI-Call\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model_chat,\n",
        "        temperature=temperature,\n",
        "        max_tokens=min(max_answer_tokens, token_limit),\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_msg},\n",
        "            {\"role\": \"user\", \"content\": user_msg},\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "_kjBa1_GbTvN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Beispielfragen\n",
        "\n",
        "\n",
        "#answer = ask(\"Fasse die Entwicklung des Tennissports von der Pre-Open Era bis heute in 3–4 Sätzen zusammen.\")\n",
        "#answer = ask(\"Welche Spielerinnen prägten das moderne Frauentennis und wie unterschieden sie sich?\")\n",
        "#answer = ask(\"Welche jungen Spieler werden als die Zukunft des Tennissports genannt und warum?\")\n",
        "answer = ask(\"Welche Spieler gehören zu den Big Three und wodurch zeichnen sie sich jeweils aus?\")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QS0kTBKokE5P",
        "outputId": "3af18b96-d096-4f24-9947-1b0246390a6f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Die Spieler der Big Three sind Roger Federer, Rafael Nadal und Novak Djokovic. Roger Federer zeichnet sich durch seinen butterweichen Spielstil und die schönste einhändige Rückhand aus; er gewann 20 Grand Slams und dominierte auf Rasenplätzen. Rafael Nadal, der König des Sandplatzes, ist bekannt für seine unermüdliche Energie und seinen Kampfgeist, was ihm 14 French Open Titel einbrachte. Novak Djokovic ist für seine außergewöhnliche mentale Stärke bekannt und hat zahlreiche Rekorde gebrochen, darunter die meisten Wochen als Nummer eins der Welt. Gemeinsam haben sie über 60 Grand Slam Titel gewonnen und zahlreiche unvergessliche Duelle ausgetragen.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agentensysteme"
      ],
      "metadata": {
        "id": "5EIp6c1Z12lB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agent 1 – Structured Output**\n",
        "\n",
        "Dieser Agent extrahiert strukturierte Informationen aus dem Transkript und gibt sie als JSON strukturiert zurück.\n",
        "\n",
        "Das Besondere: Die Modellantwort wird auf ein festgelegtes Schema gezwungen und anschließend validiert. Dadurch entsteht ein Output, der sowohl für Menschen lesbar (Tabelle) als auch maschinell weiterverarbeitbar ist.\n",
        "\n",
        "Ablauf im Code:\n",
        "1. Kontextaufbau\n",
        "    - Mit retrieve_top_k(...) werden die semantisch relevantesten Chunks zur Nutzerfrage geholt\n",
        "    - Chunks werden zu einem Kontextstring (ctx) zusammengefügt, getrennt durch ---.   \n",
        "    → So „sieht“ das Modell nur die Textpassagen, die wahrscheinlich relevant sind.\n",
        "\n",
        "2. Schema-Vorgabe\n",
        "- Es wird ein JSON-Schema (schema_hint) definiert:\n",
        "  - type: array → die Ausgabe muss eine Liste von Objekten sein\n",
        "  - properties → welche Felder erwartet werden (z. B. \"entity\", \"category\", \"evidence\")\n",
        "  - max_items → begrenzt die Anzahl der Einträge\n",
        "\n",
        "- Diese Vorgabe wird ins Prompt eingebaut, damit das Modell gezwungen ist, die Antwort strikt im JSON-Format zu liefern.\n",
        "\n",
        "3. System- & User-Message\n",
        "- System-Message: Weist das Modell strikt an, nur JSON ohne Prosa oder Markdown zurückzugeben.\n",
        "- User-Message: Enthält den zusammengesetzten Kontext, die Frage, die Aufgabenbeschreibung und das informelle Schema.\n",
        "\n",
        "\n",
        "4. LLM-Call (client.chat.completions.create)\n",
        "- Verwendet dein definiertes Modell (model_chat) und temperature=0 für deterministische Ergebnisse.\n",
        "- max_tokens ist begrenzt, damit die Antwort nicht zu lang wird.\n",
        "\n",
        "5. Parsing & Validierung\n",
        "- Mit json.loads(raw) wird die Antwort in ein Python-Objekt umgewandelt.\n",
        "- Falls das Modell nicht exakt im Schema antwortet → Fehlerbehandlung mit Fallback.\n",
        "- Für jeden Eintrag werden die Felder entity, category, evidence extrahiert und bereinigt.\n",
        "\n",
        "6. Tabellarische Ausgabe\n",
        "- Die Liste wird in ein pandas.DataFrame umgewandelt.\n",
        "\n"
      ],
      "metadata": {
        "id": "arXcq6TJs2Gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agent 1: Structured Output\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def agent_structured_summary(question: str,\n",
        "                             fields=(\"entity\", \"category\", \"evidence\"),\n",
        "                             max_items: int = 5):\n",
        "    \"\"\"\n",
        "    Structured Output Agent:\n",
        "    - Holt Top-k Chunks (semantisches Retrieval)\n",
        "    - Erzwingt JSON-Schema in der Modellantwort\n",
        "    - Validiert JSON; gibt (list[dict], DataFrame) zurück\n",
        "    \"\"\"\n",
        "    # Kontext via semantisches Retrieval\n",
        "    retrieved = retrieve_top_k(question, embeddings_transcript, chunks, top_k)\n",
        "    ctx = \"\\n\\n---\\n\\n\".join([txt for _, _, txt in retrieved])\n",
        "\n",
        "    # Striktes JSON-Prompt\n",
        "    schema_hint = {\n",
        "        \"type\": \"array\",\n",
        "        \"max_items\": max_items,\n",
        "        \"items\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {f: {\"type\": \"string\"} for f in fields},\n",
        "            \"required\": list(fields),\n",
        "            \"additionalProperties\": False\n",
        "        }\n",
        "    }\n",
        "    system_msg = (\n",
        "        \"You output ONLY valid JSON that matches the given schema. \"\n",
        "        \"No prose, no markdown, no comments.\"\n",
        "    )\n",
        "    user_msg = (\n",
        "        f\"Context:\\n{ctx}\\n\\n\"\n",
        "        f\"Task: Extract up to {max_items} items relevant to the question.\\n\"\n",
        "        f\"Question: {question}\\n\\n\"\n",
        "        f\"JSON schema (informal): {json.dumps(schema_hint)}\\n\"\n",
        "        f\"Return ONLY a JSON array, e.g. \"\n",
        "        f\"[{{\\\"{fields[0]}\\\": \\\"...\\\", \\\"{fields[1]}\\\": \\\"...\\\", \\\"{fields[2]}\\\": \\\"...\\\"}}]\"\n",
        "    )\n",
        "\n",
        "    # LLM-Call\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model_chat,\n",
        "        temperature=0,\n",
        "        max_tokens=min(400, token_limit),\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_msg},\n",
        "            {\"role\": \"user\", \"content\": user_msg},\n",
        "        ],\n",
        "    )\n",
        "    raw = resp.choices[0].message.content.strip()\n",
        "\n",
        "    # Validieren/Parsen\n",
        "    try:\n",
        "        data = json.loads(raw)\n",
        "        if not isinstance(data, list):\n",
        "            raise ValueError(\"Top-level JSON ist kein Array.\")\n",
        "        # weiche Schema-Prüfung\n",
        "        cleaned = []\n",
        "        for item in data[:max_items]:\n",
        "            row = {k: str(item.get(k, \"\")).strip() for k in fields}\n",
        "            cleaned.append(row)\n",
        "    except Exception as e:\n",
        "        # Fallback: leere Liste zurückgeben mit Fehlerhinweis\n",
        "        cleaned = []\n",
        "        print(\"JSON-Parse/Schema-Fehler:\", e)\n",
        "        print(\"Rohantwort war:\\n\", raw)\n",
        "\n",
        "    # Optional: Tabelle\n",
        "    df = pd.DataFrame(cleaned, columns=list(fields))\n",
        "    return cleaned, df\n",
        "\n",
        "# Beispielaufrufe\n",
        "# items, df = agent_structured_summary(\"Nenne Big Three + Kategorie + kurze Evidenz aus dem Text\")\n",
        "# display(df)\n"
      ],
      "metadata": {
        "id": "ZWtAa1VkxCmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agent 2 – Wetter-Agent**\n",
        "\n",
        "Dieser Agent kombiniert ein LLM (über den bestehenden OpenAI-Client) mit einem\n",
        "externen Tool (Open-Meteo API). Damit wird er zu einem „Agenten“, weil er nicht\n",
        "nur Text generiert, sondern eigenständig Aktionen ausführen darf.\n",
        "\n",
        "Struktur:\n",
        "1. WeatherTool\n",
        "   - Stellt ein einzelnes Tool bereit: Wetterdaten abfragen über Open-Meteo.\n",
        "   - Enthält:\n",
        "     * Geocoding (Stadtname → Koordinaten, via Open-Meteo Geocoding API)\n",
        "     * Forecast (Koordinaten → aktuelles Wetter, Temperatur, Wind etc.)\n",
        "   - Definiert zudem eine Tool-Spezifikation (JSON-Schema), die dem LLM erklärt,\n",
        "     wie das Tool aufzurufen ist (Name, Beschreibung, Parameter).\n",
        "\n",
        "2. WeatherAgent\n",
        "   - Orchestrator, der Nachrichten zwischen User, LLM und Tool koordiniert.\n",
        "   - Ablauf:\n",
        "     a) User gibt Eingabe („Wie ist das Wetter in Mailand?“).\n",
        "     b) Agent ruft das LLM an (client.chat.completions.create), übergibt System-\n",
        "        Prompt + User-Eingabe + Tool-Spezifikation.\n",
        "     c) Falls das LLM Tool-Aufrufe zurückgibt (Function Calling), führt der Agent\n",
        "        die Python-Funktion (WeatherTool.run) aus und hängt deren Ergebnisse als\n",
        "        „tool“-Nachrichten an den Nachrichtenverlauf an.\n",
        "     d) Der Agent fragt das LLM erneut, bis es eine finale Antwort generiert.\n",
        "   - Es gibt eine Schleife mit max. 4 Runden, um mehrfaches Nachfragen/Toolen\n",
        "     zu ermöglichen.\n",
        "\n",
        "3. CLI-Demo (main)\n",
        "   - Startet den Agenten im Terminal.\n",
        "   - User kann interaktiv Fragen stellen.\n",
        "   - Der Agent antwortet, nutzt dabei bei Bedarf das Tool.\n",
        "\n",
        "Besonderheiten:\n",
        "- Der API-Key wird **nicht** im Agenten gehandhabt. Stattdessen nutzt er den\n",
        "  bestehenden OpenAI-Client (`client`) und das konfigurierte Modell (`model_chat`).\n",
        "- Das Tool nutzt frei verfügbare Open-Meteo-Endpunkte, kein Key nötig.\n",
        "- Mit der Hilfsfunktion `_tool_calls_from_msg` werden die Tool-Calls des SDK\n",
        "  normalisiert (immer mit `type: \"function\"`), um API-Fehler zu vermeiden.\n",
        "- Der Agent liefert Antworten in normalem Text (LLM-Generierung), angereichert\n",
        "  mit echten Wetterdaten.\n"
      ],
      "metadata": {
        "id": "m4TR1dYE1MCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Agent 2 – Wetter-Agent\n",
        "\n",
        "import json\n",
        "import requests\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "\n",
        "# Wetter-Tool (Open-Meteo)\n",
        "class WeatherTool:\n",
        "    \"\"\"Holt aktuelle Wetterdaten über Open-Meteo (kein API-Key nötig).\"\"\"\n",
        "\n",
        "    GEO_URL = \"https://geocoding-api.open-meteo.com/v1/search\"\n",
        "    WX_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
        "\n",
        "    def spec(self) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": \"weather\",\n",
        "                \"description\": \"Gibt aktuelles Wetter für eine Stadt zurück.\",\n",
        "                \"parameters\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"city\": {\"type\": \"string\", \"description\": \"Stadtname\"},\n",
        "                        \"country\": {\"type\": \"string\", \"description\": \"optional: Land (2-Letter Code)\"},\n",
        "                    },\n",
        "                    \"required\": [\"city\"],\n",
        "                },\n",
        "            },\n",
        "        }\n",
        "\n",
        "    def run(self, args: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        city = str(args.get(\"city\", \"\")).strip()\n",
        "        country = str(args.get(\"country\", \"\")).strip() or None\n",
        "        if not city:\n",
        "            return {\"error\": \"city missing\"}\n",
        "\n",
        "        try:\n",
        "            # Stadt → Koordinaten\n",
        "            params = {\"name\": city, \"count\": 1}\n",
        "            if country:\n",
        "                params[\"country\"] = country\n",
        "            geo_resp = requests.get(self.GEO_URL, params=params, timeout=10)\n",
        "            geo_resp.raise_for_status()\n",
        "            geo = geo_resp.json()\n",
        "            if not geo.get(\"results\"):\n",
        "                return {\"error\": \"city not found\"}\n",
        "            r0 = geo[\"results\"][0]\n",
        "            lat, lon = r0[\"latitude\"], r0[\"longitude\"]\n",
        "\n",
        "            # Koordinaten → aktuelles Wetter\n",
        "            wx_resp = requests.get(\n",
        "                self.WX_URL,\n",
        "                params={\"latitude\": lat, \"longitude\": lon, \"current_weather\": True},\n",
        "                timeout=10,\n",
        "            )\n",
        "            wx_resp.raise_for_status()\n",
        "            current = wx_resp.json().get(\"current_weather\", {})\n",
        "\n",
        "            return {\n",
        "                \"city\": r0.get(\"name\"),\n",
        "                \"country\": r0.get(\"country_code\"),\n",
        "                \"latitude\": lat,\n",
        "                \"longitude\": lon,\n",
        "                \"temperature_c\": current.get(\"temperature\"),\n",
        "                \"wind_speed\": current.get(\"windspeed\"),\n",
        "                \"wind_dir\": current.get(\"winddirection\"),\n",
        "                \"time\": current.get(\"time\"),\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": f\"weather failed: {e}\"}\n",
        "\n",
        "\n",
        "# Function-Calling Spezifikation\n",
        "def weather_tool_spec() -> Dict[str, Any]:\n",
        "    return WeatherTool().spec()\n",
        "\n",
        "# Wetter-Agent (nutzt deinen bestehenden `client` & `model_chat`)\n",
        "class WeatherAgent:\n",
        "    def __init__(self, client, model: str, temperature: float = 0):\n",
        "        \"\"\"\n",
        "        client  : dein OpenAI-Client (z. B. from openai import OpenAI; client = OpenAI())\n",
        "        model   : Modellname (z. B. \"gpt-4o-mini\")\n",
        "        \"\"\"\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "        self.temperature = temperature\n",
        "        self.tool = WeatherTool()\n",
        "        self.system_prompt = \"Du bist ein Assistent, der bei Wetterfragen das Wetter-Tool nutzt.\"\n",
        "\n",
        "    def _tool_calls_from_msg(self, msg) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Kompatibilitätsschicht:\n",
        "        - Neues OpenAI-SDK: msg.tool_calls (Objekte)\n",
        "        - Manche Rückgaben evtl. als dict\n",
        "        Gibt eine Liste normalisierter tool_calls zurück: [{id, function:{name, arguments}}]\n",
        "        \"\"\"\n",
        "        tc = getattr(msg, \"tool_calls\", None)\n",
        "        if not tc:\n",
        "            return []\n",
        "        norm = []\n",
        "        for t in tc:\n",
        "            # OpenAI SDK\n",
        "            if hasattr(t, \"id\"):\n",
        "                norm.append({\n",
        "                    \"id\": t.id,\n",
        "                    \"type\": \"function\",\n",
        "                    \"function\": {\n",
        "                        \"name\": t.function.name,\n",
        "                        \"arguments\": t.function.arguments or \"{}\",\n",
        "                    },\n",
        "                })\n",
        "            else:\n",
        "                # dict-ähnlich → sicherstellen, dass 'type' gesetzt ist\n",
        "                d = dict(t)\n",
        "                d.setdefault(\"type\", \"function\")\n",
        "                norm.append(d)\n",
        "        return norm\n",
        "\n",
        "    def run(self, user_input: str) -> str:\n",
        "        messages: List[Dict[str, Any]] = [\n",
        "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_input},\n",
        "        ]\n",
        "\n",
        "        for _ in range(4):\n",
        "            completion = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                temperature=self.temperature,\n",
        "                messages=messages,\n",
        "                tools=[self.tool.spec()],\n",
        "                tool_choice=\"auto\",\n",
        "            )\n",
        "            choice = completion.choices[0]\n",
        "            msg = choice.message\n",
        "\n",
        "            tool_calls = self._tool_calls_from_msg(msg)\n",
        "            if tool_calls:\n",
        "                # Die vom Modell gewünschten Tool-Aufrufe ausführen\n",
        "                # und als 'tool'-Nachrichten antworten\n",
        "                messages.append({\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": None,\n",
        "                    \"tool_calls\": tool_calls,\n",
        "                })\n",
        "                for tc in tool_calls:\n",
        "                    name = tc[\"function\"][\"name\"]\n",
        "                    args_json = tc[\"function\"].get(\"arguments\", \"{}\")\n",
        "                    try:\n",
        "                        args = json.loads(args_json) if isinstance(args_json, str) else (args_json or {})\n",
        "                    except json.JSONDecodeError:\n",
        "                        args = {}\n",
        "                    output = self.tool.run(args)\n",
        "                    messages.append({\n",
        "                        \"role\": \"tool\",\n",
        "                        \"tool_call_id\": tc[\"id\"],\n",
        "                        \"name\": name,\n",
        "                        \"content\": json.dumps(output, ensure_ascii=False),\n",
        "                    })\n",
        "                continue  # eine weitere Runde mit angereichertem Kontext\n",
        "\n",
        "            # Finale Assistant-Antwort\n",
        "            final = msg.content\n",
        "            if final:\n",
        "                return final.strip()\n",
        "\n",
        "        return \"Keine Antwort erhalten.\"\n",
        "\n",
        "\n",
        "# CLI-Demo\n",
        "def main():\n",
        "    try:\n",
        "        client  # type: ignore  # existiert schon?\n",
        "        model_chat  # type: ignore\n",
        "    except NameError:\n",
        "        raise RuntimeError(\n",
        "            \"Bitte definiere vorher `client` (OpenAI-Client) und `model_chat`, \"\n",
        "            \"oder kommentiere den Fallback im main() ein.\"\n",
        "        )\n",
        "\n",
        "    agent = WeatherAgent(client, model_chat)\n",
        "    print(\"Wetter-Agent gestartet. Frage mich nach dem Wetter (Strg+C beendet).\\n\")\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"> \").strip()\n",
        "            if not user_input:\n",
        "                continue\n",
        "            answer = agent.run(user_input)\n",
        "            print(f\"\\nAgent: {answer}\\n\")\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nTschüss!\")\n",
        "            break\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32IQlGs55YaM",
        "outputId": "62087a09-d4f6-45fa-831d-0108ea70b3b0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wetter-Agent gestartet. Frage mich nach dem Wetter (Strg+C beendet).\n",
            "\n",
            "> Wie ist das Wetter in Mailand\n",
            "\n",
            "Agent: Das aktuelle Wetter in Mailand, Italien, ist wie folgt:\n",
            "\n",
            "- **Temperatur:** 19,4 °C\n",
            "- **Windgeschwindigkeit:** 6,0 km/h\n",
            "- **Windrichtung:** 123°\n",
            "\n",
            "Wenn du weitere Informationen benötigst, lass es mich wissen!\n",
            "\n",
            "> Brauche ich einen Regenschirm? \n",
            "\n",
            "Agent: Um dir zu sagen, ob du einen Regenschirm brauchst, benötige ich den Namen deiner Stadt oder deines Standorts. Bitte teile mir diese Information mit!\n",
            "\n",
            "> In Mailand\n",
            "\n",
            "Agent: Das aktuelle Wetter in Mailand (Italien) ist wie folgt:\n",
            "\n",
            "- **Temperatur:** 19,4 °C\n",
            "- **Windgeschwindigkeit:** 6,0 km/h\n",
            "- **Windrichtung:** 123°\n",
            "\n",
            "Wenn du weitere Informationen benötigst, lass es mich wissen!\n",
            "\n",
            "> Brauche ich einen Regenschirm in Mailand?\n",
            "\n",
            "Agent: Das aktuelle Wetter in Mailand zeigt eine Temperatur von 19,4 °C und es gibt keine Hinweise auf Regen. Daher benötigst du heute keinen Regenschirm.\n",
            "\n",
            "> Danke\n",
            "\n",
            "Agent: Gern geschehen! Wie kann ich Ihnen weiterhelfen?\n",
            "\n",
            "\n",
            "Tschüss!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agent 3 – Prompt-Chaining**\n",
        "\n",
        "Dieser Agent dient als mehrstufiger Orchestrator für Aufgaben im Tennis-Kontext. Er kombiniert:\n",
        "- LLM-Planung & Textproduktion (Prompt-Chaining)\n",
        "- faktenbasierte Antworten aus deinem RAG-System, das auf dem Tennis-Transkript (YouTube-Video) aufsetzt.\n",
        "\n",
        "So entstehen strukturierte, nachvollziehbare Ergebnisse: Planung → Ausführung (inkl. RAG) → Review → sauberer Markdown-Output.\n",
        "\n",
        "\n",
        "Abhängigkeiten & Kontext\n",
        "\n",
        "- Nutzen von bereits initialisierten OpenAI-Client (client) + Modell (model_chat) sowie Globale Settings (temperature, token_limit)\n",
        "- Verwendung der RAG-Funktion ask(query):\n",
        "ask(...) macht Retrieval → baut Kontext → ruft LLM → liefert die Antwort nur auf Basis des Transkripts\n",
        "\n",
        "Architektur im Überblick\n",
        "1. Helpers\n",
        "  - _chat_llm(...): Dünner Wrapper um client.chat.completions.create(...) für normale Textausgaben (z. B. Finale).\n",
        "  - _chat_json(...): Erzwingt wohlgeformtes JSON (über response_format={\"type\": \"json_object\"}); hat Fallbacks (Regex-Extraktion + sichere Defaults), damit die Pipeline nicht bei JSON-Fehlern stoppt.\n",
        "  - _extract_json(...): Schneidet JSON aus einer gemischten Antwort (erste { bis letzte }).\n",
        "\n",
        "2. Agent-Klasse TennisRAGAgent\n",
        "Der Agent kapselt fünf Phasen (Methoden) und eine run(...)-Pipeline:\n",
        "  - understand(task): Extrahiert Ziele, Kriterien, Annahmen, Constraints als JSON.\n",
        "  → sorgt für klaren Auftrag und Bewertungsmaßstäbe.\n",
        "\n",
        "  - plan(task, understanding): Baut einen   Schritte-Plan (max. 6). Jeder Schritt hat mode:\n",
        "    - \"rag\" → Schritt muss die Quelle nutzen (führt später ask(query) aus)\n",
        "    - \"llm\" → reicht LLM-Begründung/Struktur/Meta\n",
        "  Dazu: query (für RAG) oder instruction (für LLM) + output_spec (erwartete Ausgabeform).\n",
        "\n",
        "- execute(task, plan, understanding): Führt alle Schritte sequenziell aus:\n",
        "    - RAG: ruft ask(query) auf (bei Liste von Queries: iteriert und kombiniert Ergebnisse).\n",
        "    - LLM: generiert Text anhand instruction und output_spec. Die Zwischenergebnisse werden gesammelt (pro Schritt: step_id, title, mode, output).\n",
        "\n",
        "- review(...): Strenger Self-Check als JSON (passed, issues, fixes, quality_score_0_10).\n",
        "Kriterien: Quellenkonsistenz (bei RAG), Klarheit, Kürze, Logik.\n",
        "\n",
        "- finalize(...): Baut kompakten Markdown-Output (Deutsch, klare Gliederung, Wortlimit).\n",
        "Integriert nötige Fixes aus review.\n",
        "\n",
        "- run(...): Orchestriert die komplette Kette:\n",
        "    1. understand → 2. plan → 3. execute → 4. review → 5. finalize und gibt ein Dictionary mit allen Zwischenergebnissen + final_markdown zurück.\n",
        "\n",
        "Interaktion Agent und RAG\n",
        "- In der Plan-Phase erzwingt der Prompt, dass bei faktenpflichtigen Schritten mode: \"rag\" gesetzt wird und eine konkrete query definiert ist.\n",
        "\n",
        "- In der Execute-Phase erkennt der Agent mode == \"rag\" und ruft ask(query) auf → damit wird wirklich das Transkript befragt (semantisches Retrieval, kontrollierter Kontext, Antwort nur aus Quelle).\n",
        "- Für Meta-Aufgaben (z. B. Struktur bündeln, kurze Einordnung) nutzt der Agent mode == \"llm\".\n",
        "\n",
        "Ergebnis: Transparenz, Nachvollziehbarkeit und Faktenbindung dort, wo es wichtig ist.\n",
        "\n",
        "Token-Kontrolle & Robustheit\n",
        "- _chat_llm wählt ein konservatives max_tokens-Default (abhängig vom globalen token_limit), damit man nicht aus Versehen über Budget geht.\n",
        "- JSON-Robustheit (_chat_json):\n",
        "    - Erzwingt JSON-Antworten (falls Modell/SDK das Feature unterstützt).\n",
        "    - Hat Fallbacks (Regex-Extraktion, sichere leere Defaults), damit die Pipeline nicht mit JSONDecodeError abbricht.\n",
        "\n",
        "- Wortlimit im finalize(...): Der finale Markdown bleibt kurz & lesbar; man kann max_words im run(...) setzen.\n"
      ],
      "metadata": {
        "id": "BIa-1APKAQ4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agent 3 – Prompt-Chaining mit RAG-Interaktion\n",
        "\n",
        "\n",
        "import re\n",
        "import json\n",
        "from typing import List, Dict, Any, Optional\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "\n",
        "def _extract_json(s: str) -> str:\n",
        "    \"\"\"Versucht JSON aus einem beliebigen String zu schneiden (erstes { bis letztes }).\"\"\"\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    i = s.find(\"{\")\n",
        "    j = s.rfind(\"}\")\n",
        "    return s[i:j+1] if (i != -1 and j != -1 and j > i) else s\n",
        "\n",
        "def _chat_llm(\n",
        "    messages: List[Dict[str, str]],\n",
        "    *,\n",
        "    temp: Optional[float] = None,\n",
        "    max_tokens: Optional[int] = None\n",
        ") -> str:\n",
        "    \"\"\"Normaler Chat (Text). Nutzt globale Variablen aus deinem Setup.\"\"\"\n",
        "    t = temperature if temp is None else temp\n",
        "    # Budget, um Überschreitungen zu vermeiden\n",
        "    mt = max_tokens if max_tokens is not None else min(800, max(128, token_limit - 1200))\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model_chat,\n",
        "        temperature=t,\n",
        "        max_tokens=mt,\n",
        "        messages=messages,\n",
        "    )\n",
        "    return (resp.choices[0].message.content or \"\").strip()\n",
        "\n",
        "def _chat_json(\n",
        "    messages: List[Dict[str, str]],\n",
        "    *,\n",
        "    temp: float = 0.0,\n",
        "    max_tokens: Optional[int] = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    JSON-erzwungener Chat:\n",
        "      1) response_format={\"type\":\"json_object\"} (falls unterstützt)\n",
        "      2) Fallback: JSON per _extract_json ausschneiden und parsen\n",
        "      3) Fallback: {} zurückgeben statt Exception\n",
        "    \"\"\"\n",
        "    mt = max_tokens if max_tokens is not None else min(800, max(128, token_limit - 1200))\n",
        "    try:\n",
        "        resp = client.chat.completions.create(\n",
        "            model=model_chat,\n",
        "            temperature=temp,\n",
        "            max_tokens=mt,\n",
        "            messages=messages,\n",
        "            response_format={\"type\": \"json_object\"},\n",
        "        )\n",
        "        raw = (resp.choices[0].message.content or \"\").strip()\n",
        "        return json.loads(raw) if raw else {}\n",
        "    except Exception:\n",
        "        raw = _chat_llm(messages, temp=temp, max_tokens=mt)\n",
        "        try:\n",
        "            return json.loads(_extract_json(raw)) if raw else {}\n",
        "        except json.JSONDecodeError:\n",
        "            return {}\n",
        "\n",
        "\n",
        "class TennisRAGAgent:\n",
        "    \"\"\"\n",
        "    Prompt-Chaining für Tennis – mit expliziter RAG-Interaktion.\n",
        "    Schritt-Schema (Plan):\n",
        "    {\n",
        "      \"id\": \"S1\",\n",
        "      \"title\": \"Kurzer Titel\",\n",
        "      \"mode\": \"rag\" | \"llm\",\n",
        "      \"query\": \"RAG-Frage(n) ...\"          # für mode=\"rag\" (str oder Liste[str])\n",
        "      \"instruction\": \"LLM-Anweisung ...\"   # für mode=\"llm\"\n",
        "      \"output_spec\": \"Was genau erwartet wird (knapp).\"\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Verständnis\n",
        "    def understand(self, task: str) -> Dict[str, Any]:\n",
        "        msgs = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a tennis RAG task analyst. Be precise, concise, German output.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"\"\"Aufgabe: {task}\n",
        "\n",
        "Extrahiere NUR als JSON:\n",
        "- goals: 1–3 Hauptziele im Tennis-Kontext\n",
        "- criteria: messbare Erfolgs-/Akzeptanzkriterien\n",
        "- assumptions: sinnvolle Annahmen (z. B. Fokus Herren, Zeitraum, Turniere)\n",
        "- constraints: relevante Nebenbedingungen (Stil: prägnant; Deutsch; Länge; Struktur)\n",
        "\"\"\"}]\n",
        "        data = _chat_json(msgs, temp=0.1)\n",
        "        return {\n",
        "            \"goals\": data.get(\"goals\", []),\n",
        "            \"criteria\": data.get(\"criteria\", []),\n",
        "            \"assumptions\": data.get(\"assumptions\", []),\n",
        "            \"constraints\": data.get(\"constraints\", []),\n",
        "        }\n",
        "\n",
        "    # 2) Plan\n",
        "    def plan(self, task: str, understanding: Dict[str, Any], max_steps: int = 6) -> List[Dict[str, Any]]:\n",
        "        msgs = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a pragmatic planner for a Tennis RAG. Return ONLY a JSON array of steps.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"\"\"Aufgabe: {task}\n",
        "Kontext (JSON):\n",
        "{json.dumps(understanding, ensure_ascii=False, indent=2)}\n",
        "\n",
        "Erzeuge höchstens {max_steps} Schritte. Jeder Schritt:\n",
        "- id (S1..), title\n",
        "- mode: \"rag\" (nutzt ask(query)) oder \"llm\"\n",
        "- query (für \"rag\") ODER instruction (für \"llm\")\n",
        "- output_spec (knapp definieren)\n",
        "\n",
        "Nutze \"rag\" bei faktenpflichtigen Passagen (Epochen/Spieler/Rivalitäten, Turniere, historische Aussagen).\n",
        "Antworte NUR mit JSON-Liste.\n",
        "\"\"\"}]\n",
        "        data = _chat_json(msgs, temp=0.1)\n",
        "        if isinstance(data, dict):  # falls das Modell ein Objekt statt Liste liefert\n",
        "            data = [data] if data else []\n",
        "        return data if isinstance(data, list) else []\n",
        "\n",
        "    # Ausführen\n",
        "    def execute(self, task: str, plan: List[Dict[str, Any]], understanding: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
        "        results: List[Dict[str, Any]] = []\n",
        "        for step in plan:\n",
        "            mode = (step.get(\"mode\") or \"\").lower().strip()\n",
        "            output = \"\"\n",
        "\n",
        "            if mode == \"rag\":\n",
        "                q = step.get(\"query\")\n",
        "                if isinstance(q, list):\n",
        "                    parts = []\n",
        "                    for subq in q:\n",
        "                        try:\n",
        "                            parts.append(ask(subq))  # deine RAG-Pipeline\n",
        "                        except Exception as e:\n",
        "                            parts.append(f\"[RAG-Fehler: {e}]\")\n",
        "                    output = \"\\n\\n\".join(parts)\n",
        "                else:\n",
        "                    try:\n",
        "                        output = ask(q or \"\")\n",
        "                    except Exception as e:\n",
        "                        output = f\"[RAG-Fehler: {e}]\"\n",
        "\n",
        "            elif mode == \"llm\":\n",
        "                instruction = step.get(\"instruction\") or \"\"\n",
        "                spec = step.get(\"output_spec\") or \"Knappe, präzise Antwort.\"\n",
        "                msgs = [\n",
        "                    {\"role\": \"system\", \"content\": \"You are a concise tennis analyst. German output.\"},\n",
        "                    {\"role\": \"user\", \"content\": f\"Aufgabe: {task}\\nAnweisung: {instruction}\\nErwartung: {spec}\"},\n",
        "                    {\"role\": \"assistant\", \"content\": f\"Zwischenergebnisse:\\n{json.dumps(results, ensure_ascii=False)[:1200]}\"},\n",
        "                ]\n",
        "                output = _chat_llm(msgs, temp=0.2)\n",
        "\n",
        "            else:\n",
        "                output = \"[Planfehler: mode muss 'rag' oder 'llm' sein]\"\n",
        "\n",
        "            results.append({\n",
        "                \"step_id\": step.get(\"id\"),\n",
        "                \"title\": step.get(\"title\"),\n",
        "                \"mode\": mode,\n",
        "                \"output\": output\n",
        "            })\n",
        "        return results\n",
        "\n",
        "    # 4) Review\n",
        "    def review(self, task: str, understanding: Dict[str, Any], plan: List[Dict[str, Any]], exec_results: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        msgs = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a meticulous reviewer for Tennis RAG outputs. Be strict and specific.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"\"\"Task: {task}\n",
        "Understanding:\n",
        "{json.dumps(understanding, ensure_ascii=False, indent=2)}\n",
        "Plan:\n",
        "{json.dumps(plan, ensure_ascii=False, indent=2)}\n",
        "Results:\n",
        "{json.dumps(exec_results, ensure_ascii=False, indent=2)}\n",
        "\n",
        "Return ONLY JSON:\n",
        "{{\"passed\": true/false, \"issues\": [\"...\"], \"fixes\": [\"...\"], \"quality_score_0_10\": <int>}}\n",
        "Fokussiere auf: Konsistenz mit Quelle (RAG-Schritte), Klarheit, Kürze, logische Struktur.\n",
        "\"\"\"}]\n",
        "        data = _chat_json(msgs, temp=0)\n",
        "        return {\n",
        "            \"passed\": bool(data.get(\"passed\", False)),\n",
        "            \"issues\": data.get(\"issues\", []),\n",
        "            \"fixes\": data.get(\"fixes\", []),\n",
        "            \"quality_score_0_10\": data.get(\"quality_score_0_10\", 0),\n",
        "        }\n",
        "\n",
        "    # 5) Finalisieren (Markdown)\n",
        "    def finalize(\n",
        "        self,\n",
        "        task: str,\n",
        "        understanding: Dict[str, Any],\n",
        "        plan: List[Dict[str, Any]],\n",
        "        exec_results: List[Dict[str, Any]],\n",
        "        review: Dict[str, Any],\n",
        "        *,\n",
        "        max_words: int = 280,\n",
        "        style: str = \"Deutsch, prägnant, klar gegliedert, keine Ausschmückung.\"\n",
        "    ) -> str:\n",
        "        msgs = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a precise writer. Integrate necessary fixes; output ONLY Markdown.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"\"\"Task: {task}\n",
        "Understanding:\n",
        "{json.dumps(understanding, ensure_ascii=False, indent=2)}\n",
        "Plan:\n",
        "{json.dumps(plan, ensure_ascii=False, indent=2)}\n",
        "Results (Auszüge):\n",
        "{json.dumps(exec_results, ensure_ascii=False, indent=2)[:2000]}\n",
        "Review:\n",
        "{json.dumps(review, ensure_ascii=False, indent=2)}\n",
        "\n",
        "Stil: {style}\n",
        "Limit: ≤ {max_words} Wörter.\n",
        "Gib NUR Markdown zurück (Überschriften + knappe Bullet-Points).\n",
        "\"\"\"}]\n",
        "        return _chat_llm(msgs, temp=0.25, max_tokens=min(900, token_limit - 1024))\n",
        "\n",
        "    # Pipeline\n",
        "    def run(self, task: str, *, max_steps: int = 6, max_words: int = 280, style: str = \"Deutsch, prägnant, klar gegliedert.\") -> Dict[str, Any]:\n",
        "        understanding = self.understand(task)\n",
        "        plan = self.plan(task, understanding, max_steps=max_steps)\n",
        "        exec_results = self.execute(task, plan, understanding)\n",
        "        review = self.review(task, understanding, plan, exec_results)\n",
        "        final_md = self.finalize(task, understanding, plan, exec_results, review, max_words=max_words, style=style)\n",
        "        return {\n",
        "            \"understanding\": understanding,\n",
        "            \"plan\": plan,\n",
        "            \"results\": exec_results,\n",
        "            \"review\": review,\n",
        "            \"final_markdown\": final_md,\n",
        "        }\n",
        "\n",
        "# Beispielausführung\n",
        "agent = TennisRAGAgent()\n",
        "task = (\"Erstelle eine kompakte Übersicht zur Rivalität der neuen Spitze im Herren-Tennis \"\n",
        "         \"(Post-'Big Three'), inkl. kurzer Epochen-Einordnung und 3 Q&A-Antworten aus der Quelle (RAG).\")\n",
        "out = agent.run(task, max_steps=5, max_words=260)\n",
        "display(Markdown(out[\"final_markdown\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "IYCE9uAH-N2W",
        "outputId": "76677139-7ca3-4644-a028-16b2f2b3bff6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Übersicht zur Rivalität der neuen Spitze im Herren-Tennis (Post-'Big Three')\n\n## 1. Identifikation der neuen Top-Spieler\n- **Carlos Alcaraz**: 2 Grand-Slam-Titel, ATP-Ranking: 1\n- **Daniil Medvedev**: 1 Grand-Slam-Titel, ATP-Ranking: 3\n- **Jannik Sinner**: 0 Grand-Slam-Titel, ATP-Ranking: 4\n- **Stefanos Tsitsipas**: 0 Grand-Slam-Titel, ATP-Ranking: 5\n\n## 2. Analyse der Rivalität\n- **Direkte Duelle**:\n  - Alcaraz vs. Medvedev: 2-1\n  - Alcaraz vs. Sinner: 3-0\n  - Medvedev vs. Sinner: 1-1\n  - Tsitsipas vs. Alcaraz: 1-2\n\n## 3. Bewertung der Auswirkungen auf die Tennislandschaft\n- Die Rivalität zwischen Alcaraz, Medvedev, Sinner und Tsitsipas bringt frischen Wind in die Tenniswelt.\n- Neue Spielstile und Strategien prägen das Spiel, was zu spannenden Matches führt.\n- Die Ära nach den 'Big Three' fördert jüngere Talente und erhöht das Interesse an Herren-Tennis.\n\n## 4. Epochen-Einordnung\n- **'Big Three'-Ära**: Dominanz von Federer, Nadal und Djokovic mit zahlreichen Rekorden.\n- **Post-'Big Three'-Ära**: Aufstieg junger Spieler, die um die Spitze kämpfen und die Tradition herausfordern.\n\n## 5. Q&A zur Rivalität\n- **Frage 1**: Wer sind die Hauptakteure der neuen Rivalität?\n  - **Antwort**: Carlos Alcaraz, Daniil Medvedev, Jannik Sinner und Stefanos Tsitsipas.\n  \n- **Frage 2**: Wie beeinflussen diese Spieler die Zuschauerzahlen?\n  - **Antwort**: Ihre aufregenden Matches ziehen neue Zuschauer an und revitalisieren das Interesse am Tennis.\n  \n- **Frage 3**: Gibt es Parallelen zur 'Big Three'-Ära?\n  - **Antwort**: Ja, die Rivalität zeigt ähnliche Spannungen und Dramatik, jedoch mit einem jüngeren, dynamischeren Ansatz."
          },
          "metadata": {}
        }
      ]
    }
  ]
}
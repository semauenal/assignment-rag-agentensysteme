{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval-Augmented Generation (RAG)"
      ],
      "metadata": {
        "id": "Qo6lPJ33dHDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2 Implementierung eines KI-Systems mit RAG  und Agentenkomponenten**\n",
        "\n",
        "**2.1 RAG-System**\n",
        "\n",
        "Wie bereits im ersten Teil des Assignments beschrieben, kommen RAG-Systeme (Retrieval-Augmented Generation) immer dann zum Einsatz, wenn externe Wissensquellen in ein Sprachmodell eingebunden werden sollen. Die Motivation liegt darin, dass ein LLM allein nur auf seinem Trainingswissen basiert, während sich durch RAG zusätzlich aktuelle oder spezialisierte Informationen berücksichtigen lassen. Das Vorgehen umfasst im Wesentlichen die Indexierung externer Daten, das semantische Retrieval relevanter Inhalte und die Generierung einer Antwort durch das LLM. Dieses Notebook zeigt exemplarisch, wie ein solches System aufgebaut wird und wie die einzelnen Bausteine zusammenwirken.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "xWNni9nfV5Fy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Auswahl der externen Quelle**\n",
        "\n",
        "Da eines meiner Hobbys das Anschauen von Sport ist, insbesondere **Tennis**, und wir uns derzeit in der Hochsaison befinden, in der gerade Wimbledon zu Ende gegangen ist und nun Cincinnati und die US Open direkt hintereinander folgen, habe ich beschlossen, diese Aufgabe auf diesen Bereich zu konzentrieren. Darüber hinaus hat sich nach der Ära der „Big Three“ eine neue Rivalität im Herrentennis entwickelt, die noch mehr Aufmerksamkeit und Begeisterung für diesen Sport mit sich bringt.  \n",
        "\n",
        "Auf der Suche nach geeigneten Videos oder Dokumenten mit interessanten und relevanten Informationen für diese RAG-Aufgabe bin ich auf ein sehr gutes YouTube-Video gestoßen:  \n",
        "**„Every TENNIS ERA Explained In 10 Minutes”** ([Link](https://www.youtube.com/watch?v=dxg1-JXKG5A)).  \n",
        "\n",
        "Ich habe dieses Video transkribiert und die Transkription in mein GitHub-Repository hochgeladen ([Raw-Link](https://raw.githubusercontent.com/semauenal/assignment-rag-agentensysteme/refs/heads/main/Transcript_Every_Tennis_Era_Explained.txt)). Die Transkription umfasst **1.508 Wörter** und dient als externe Wissensquelle für das in diesem Notizbuch implementierte RAG-System.  \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "FYh8gL8QZ2hT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Erklärungen zum Code**\n",
        "\n",
        "Bevor endgültig der Code zum RAG-System gestartet wird, soll nochmal kurz erklärt werden, auf was Wert gelegt wird beim Implementieren des RAGs.\n",
        "\n",
        "- Kommentare: Es wird versucht, verständliche, aber nicht zu viele Kommentare zu setzen.\n",
        "\n",
        "- Erklären von Abstrahierung und Frameworks bzw. Funktionen: Vorhandene Abstraktionen oder Framework-Aufrufe werden kurz erläutert, damit klar ist, was im Hintergrund passiert.\n",
        "\n",
        "- Immer wieder Print-Ausgaben: Zur Kontrolle werden Zwischenergebnisse ausgegeben, um den Ablauf und die Daten zu prüfen.\n",
        "\n",
        "- Tokennutzung / Limits: Durch Parameter wie max_tokens und die Auswahl weniger Chunks wird der Tokenverbrauch begrenzt und kontrollierbar gehalten.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Q2bWkN-ZnYTw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verbindung zu OpenAI**\n",
        "\n",
        "In diesem Abschnitt wird die Verbindung zur OpenAI-API hergestellt. Dafür wird der API-Schlüssel aus den Secrets geladen.\n",
        "\n",
        "Nutzung der OpenAI-Python-Bibliothek:\n",
        "- Zugriff auf OpenAI-API vereinfacht\n",
        "- Anstatt selbst HTTP-Anfragen an die Endpunkte (z. B. /v1/chat/completions, /v1/embeddings) zu schreiben, bietet die Bibliothek eine fertige Schnittstelle in Python\n",
        "- Klasse OpenAI erstellt einen Client, der mit dem API-Key authentifiziert wird\n",
        "\n",
        "  --> „Brücke“ zwischen dem Notebook und der OpenAI-Cloud dar und übernimmt die ganze low-level Arbeit der API-Kommunikation.\n",
        "\n"
      ],
      "metadata": {
        "id": "qAM2R1UMgy95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# API-Key aus Colab-Secrets laden\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "OPENAI_API_KEY = userdata.get('apikey_sm')\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "Cz0RpUUwXW3z"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modellkonfiguration**\n",
        "\n",
        "Damit später bei der Generierung des Outputs nicht ständig erneut die Konfigurationen angegeben werden müssen, werden sie hier einmal zentral definiert.\n"
      ],
      "metadata": {
        "id": "U9ADgpQgodlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Konfiguration durch Variabeln\n",
        "model_chat    = \"gpt-4o-mini\"\n",
        "temperature   = 0               # steuert Kreativität\n",
        "token_limit   = 4096            # internes Budget (Kontrolle, nicht das echte Kontextfenster)\n",
        "\n",
        "top_k         = 4               # Anzahl der Chunks, die beim Retrieval zurückgegeben werden\n"
      ],
      "metadata": {
        "id": "LdDyzXo9odEe"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transkript laden\n",
        "\n",
        "# Importieren von requests: Python-Bibliothek für HTTP-Anfragen\n",
        "import requests\n",
        "url = \"https://raw.githubusercontent.com/semauenal/assignment-rag-agentensysteme/refs/heads/main/Transcript_Every_Tennis_Era_Explained.txt\"\n",
        "transcript = requests.get(url).text\n",
        "print(transcript[:500])  # Vorschau"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmPcz58etNok",
        "outputId": "f1715d99-544a-4f31-a93c-f337e0def5ef"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿Back in the day, way before 1968, tennis wasn’t even a competition. It was more about tradition and having fun. The sport lived in black and white. And the players, well, it looked like they were heading to a party and not a professional game. To be fair to them, it wasn’t really a professional sport at the time, but those outfits were still outrageous. I’m talking about the pre-Open Era. This was one of the weirdest eras in the history of this sport. Why? Because this was the time when amateur\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chunking**\n",
        "\n",
        "Beim Aufbau eines RAG-Systems ist **Chunking** wichtig, weil große Dokumente nicht auf einmal in den Prompt passen.  \n",
        "Die Texte werden in Abschnitte zerlegt, damit sie als Embeddings berechnet und später gezielt abgerufen werden können.  \n",
        "Dabei gilt:  \n",
        "- Chunks dürfen nicht zu kurz sein, sonst geht inhaltlicher Kontext verloren.  \n",
        "- Eine **Überlappung** ist sinnvoll, damit am Rand keine relevanten Informationen abgeschnitten werden.  \n",
        "\n",
        "Hier wurde bewusst eine eigene kleine Funktion implementiert, um den Ablauf nachvollziehbar zu machen.  \n",
        "Es gibt zwar viele Frameworks (z. B. in LangChain oder LlamaIndex), die das Chunking mit einer einzigen Zeile erledigen,  \n",
        "aber durch die eigene Funktion wird deutlich, wie der Prozess intern funktioniert.\n",
        "\n",
        "\n",
        "Die Funktion `chunk_text` arbeitet auf Basis einer Schleife:  \n",
        "- Der Text wird zuerst in Wörter aufgesplittet.  \n",
        "- Mit einem Fenster von `chunk_size` Wörtern wird ein Abschnitt gebildet.  \n",
        "- Jeder Abschnitt wird als String gespeichert und in die Liste `chunks` geschrieben.  \n",
        "- Der Startindex verschiebt sich dann jeweils um `chunk_size - overlap`, sodass sich die Abschnitte leicht überlappen.  \n",
        "- Am Ende liefert die Funktion eine Liste von Textabschnitten zurück.  \n",
        "\n"
      ],
      "metadata": {
        "id": "nBcCSQgkvvq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunking\n",
        "# Text in Abschnitte von ca. 500 Wörtern mit Überlappung 30 Wörter\n",
        "def chunk_text(text, chunk_size=300, overlap=30):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(words):\n",
        "        end = start + chunk_size\n",
        "        chunk = \" \".join(words[start:end])\n",
        "        chunks.append(chunk)\n",
        "        start += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_text(transcript)\n",
        "print(\"Anzahl Chunks:\", len(chunks))\n",
        "# Vorschau der ersten 300 Zeichen des ersten Chunks\n",
        "print(\"Beispiel Chunk:\\n\", chunks[0][:300])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsBHm3-wwaBp",
        "outputId": "8ed46852-2d9d-4c36-89b5-9a9e6c724c88"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anzahl Chunks: 6\n",
            "Beispiel Chunk:\n",
            " ﻿Back in the day, way before 1968, tennis wasn’t even a competition. It was more about tradition and having fun. The sport lived in black and white. And the players, well, it looked like they were heading to a party and not a professional game. To be fair to them, it wasn’t really a professional spo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embedding**\n",
        "\n",
        "Nach dem Chunking werden die Textabschnitte in sogenannte Embeddings umgewandelt.  \n",
        "Ein Embedding ist ein dichter Vektor, der die semantische Bedeutung eines Textes in einer hochdimensionalen Zahlen­darstellung abbildet. So können Inhalte mathematisch verglichen werden (z. B. über Cosine-Similarity), was für das semantische Retrieval die Grundlage bildet.  \n",
        "\n",
        "Um die Chunks in Embeddings umzuwandeln wird der OpenAI-Client genutzt,  weil er einiges schon übernimmt, was man mit requests selbst machen müsste, wie zum Beispiel Header setzen, JSON bauen und Antworten parsen. Dadurch spart man Code und Fehlerquellen, abstrahiert aber auch nicht zu viel. LangChain wäre zwar noch kürzer, nimmt einem aber viele Details ab und versteckt den Ablauf stärker.\n",
        "\n",
        "\n",
        "Gewähltes Modell  \n",
        "\n",
        "Für die Embedding-Erstellung wird das Modell **`text-embedding-3-small`** von OpenAI genutzt.  \n",
        "- Vorteil: kostengünstig und gleichzeitig qualitativ gut genug für ein mittellanges Transkript.  \n",
        "- Output: pro Textabschnitt ein Vektor mit 1536 Dimensionen.  \n",
        "  - Jede Dimension ist ein Zahlenwert, der eine bestimmte latente Eigenschaft des Textes repräsentiert.  \n",
        "  - Man kann es sich wie einen „Punkt im 1536-dimensionalen Raum“ vorstellen, der die Bedeutung des Textes mathematisch beschreibt.  \n",
        "\n",
        "Abstraktionen im Code  \n",
        "\n",
        "- **`client.embeddings.create(...)`**:  \n",
        "  API-Aufruf an den OpenAI-Endpunkt `/v1/embeddings`.  \n",
        "  Abstrahiert werden: Authentifizierung, Aufbau der Anfrage, Versand und Empfang der JSON-Antwort.  \n",
        "- **`np.array(vector)`**:  \n",
        "  Umwandlung der zurückgegebenen Python-Liste in ein NumPy-Array → effizientere Verarbeitung.  \n",
        "- **`np.vstack(embeddings)`**:  \n",
        "Nimmt die einzelnen Embedding-Vektoren (jeder ist eine Liste mit 1536 Zahlen) und legt sie untereinander in eine Tabelle.\n",
        "Ergebnis: eine 2D-Matrix mit der Form (Anzahl Chunks, 1536).\n",
        "  - Jede Zeile = ein Chunk-Embedding\n",
        "  - Jede Spalte = eine bestimmte Dimension des Vektorraums\n",
        "\n",
        "\n",
        "Zusammenfassung  \n",
        "\n",
        "In diesem Schritt wird also für jeden Chunk ein Embedding erzeugt und zu einer gemeinsamen Matrix zusammengefügt.  \n",
        "Dies ist der zentrale Zwischenschritt, um später Anfragen mit denselben Methoden in denselben Vektorraum einzubetten und semantisch passende Chunks zurückzufinden.\n"
      ],
      "metadata": {
        "id": "F0IvFwbS74P0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Embeddings berechnen\n",
        "embeddings_transcript = []\n",
        "for chunk in chunks:\n",
        "    response = client.embeddings.create(\n",
        "        model=\"text-embedding-3-small\",\n",
        "        input=chunk\n",
        "    )\n",
        "    vector = response.data[0].embedding\n",
        "    embeddings_transcript.append(np.array(vector))\n",
        "\n",
        "embeddings_transcript = np.vstack(embeddings_transcript)\n",
        "\n",
        "# shape von NumPy-Arrays zeigt Form des Arrays an (Dimensionen)\n",
        "print(f\"Anzahl Embeddings: {embeddings_transcript.shape[0]}\")\n",
        "print(f\"Länge pro Embedding-Vektor: {embeddings_transcript.shape[1]} Dimensionen\")\n",
        "# Als Test anzeigen der ersten 10 Dimensionen des ersten Chunks\n",
        "print(embeddings_transcript[0][:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51lL5ov3SLSK",
        "outputId": "4eeed949-dbf2-4984-a8f3-c20fc686fec2"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anzahl Embeddings: 6\n",
            "Länge pro Embedding-Vektor: 1536 Dimensionen\n",
            "[-0.00377981  0.02224585 -0.00795002  0.01334467  0.04593974 -0.00183667\n",
            "  0.00059181 -0.02226005 -0.02113853  0.01954852]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Retrieval**\n",
        "\n",
        "Nach Umwandlung der Chunks in Embeddings, folgt Retrieval-Schritt. Ziel: Aus allen gespeicherten Embeddings die relevantesten Chunks zu einer Anfrage finden.\n",
        "\n",
        "Für diesen Code wurde bewusst ein experimenteller Ansatz gewählt. Statt sofort auf integrierte Lösungen zurückzugreifen, werden hier zwei Hilfsfunktionen selbst definiert. Dadurch wird die Logik nachvollziehbarer.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Cosine-Similarity**\n",
        "\n",
        "Die Cosine-Similarity kommt aus der linearen Algebra und Geometrie.\n",
        "Sie misst den Winkel zwischen zwei Vektoren.\n",
        "Je kleiner der Winkel, desto ähnlicher die Vektoren.\n",
        "- Formal:  \n",
        "\n",
        "$$\n",
        "\\cos(\\theta) = \\frac{a \\cdot b}{||a|| \\cdot ||b||}\n",
        "$$  \n",
        "\n",
        "mit $a \\cdot b$ = Skalarprodukt und $||a||$ = Länge (Norm) des Vektors.  \n",
        "\n",
        "*Wertebereich*\n",
        "- **1** → Vektoren zeigen exakt in dieselbe Richtung → Inhalte sehr ähnlich  \n",
        "- **0** → Vektoren stehen senkrecht (orthogonal) → keine Ähnlichkeit  \n",
        "- **-1** → Vektoren zeigen in entgegengesetzte Richtungen → theoretisch maximale Gegensätzlichkeit (bei Embeddings fast nie relevant)  \n",
        "\n",
        "*Umsetzung im Code*\n",
        "- `a` und `b` = zwei Vektoren (z. B. Query-Embedding und ein Chunk-Embedding)  \n",
        "- `np.dot(a, b)` = Skalarprodukt (Summe der komponentenweisen Multiplikationen)  \n",
        "- `np.linalg.norm(a)` = Länge des Vektors *a*, berechnet als Wurzel der Summe der Quadrate aller Dimensionen  \n",
        "\n",
        "- Eine kleine Konstante `1e-12` wird hinzugefügt, um Division durch Null zu vermeiden.  \n",
        "\n",
        "**Retrieval-Funktion**\n",
        "retrieve_top_k:\n",
        "- Query wird ebenfalls in ein Embedding umgewandelt\n",
        "- Query-Embedding wird mit allen Chunk-Embeddings verglichen\n",
        "- Die besten Treffer (Top-k) werden sortiert und zurückgegeben.\n"
      ],
      "metadata": {
        "id": "E9wUBAZoIVDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cosine Similarity zwischen zwei Vektoren\n",
        "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    return float(np.dot(a, b) / ((np.linalg.norm(a) * np.linalg.norm(b)) + 1e-12))\n",
        "\n",
        "# Retrieval-Funktion: Top-k ähnlichste Chunks\n",
        "def retrieve_top_k(query: str, embeddings: np.ndarray, chunks: list[str], k: int):\n",
        "    # Query → Embedding\n",
        "    embedding_response = client.embeddings.create(model=\"text-embedding-3-small\", input=query)\n",
        "    q_vec = np.array(embedding_response.data[0].embedding, dtype=np.float32)\n",
        "\n",
        "    # Ähnlichkeiten berechnen\n",
        "    sims = [cosine_similarity(q_vec, emb) for emb in embeddings]\n",
        "\n",
        "    # Sortieren & Top-k wählen\n",
        "    top_ids = np.argsort(sims)[::-1][:k]\n",
        "    return [(i, sims[i], chunks[i]) for i in top_ids]\n",
        "\n",
        "\n",
        "# Testaufruf (zur Kontrolle)\n",
        "query_test = \"Who are the Big Three in tennis?\"\n",
        "results = retrieve_top_k(query_test, embeddings_transcript, chunks, top_k)\n",
        "\n",
        "for idx, score, txt in results:\n",
        "    print(f\"Chunk {idx} | Score = {score:.3f}\\n'{txt[:200]}...'\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0ygefZvC_Nl",
        "outputId": "9d5a4ddc-0188-484a-d406-d342380c457e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 2 | Score = 0.624\n",
            "'set the tone for what was coming next, as it was the first time the best players were put up against each other for the world to watch. But what came next was something that nobody expected. After the...'\n",
            "\n",
            "Chunk 3 | Score = 0.576\n",
            "'than you’ll ever find anywhere else. But let’s not forget about one other guy who still left his mark in this era despite playing with such greats. Had he not played in the same era as the Big Three, ...'\n",
            "\n",
            "Chunk 4 | Score = 0.515\n",
            "'with a clean game and a cheeky celebration at the end. Alexander Zverev, with his massive serve and lethal forehand, also showed promise. He won an Olympic gold but is still hunting that all-important...'\n",
            "\n",
            "Chunk 1 | Score = 0.498\n",
            "'in 1968. The pros and amateurs were allowed to play together in the same tournaments. And let me say this, things got really spicy really quickly. Money started flowing into the sport. Television came...'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generierung des Outputs**\n",
        "\n",
        "Nun geht es um den letzten Teil der RAG-Pipeline. Es wurde bewusst die Variante über den **OpenAI-Client** gewählt, anstatt Frameworks wie LangChain zu nutzen.  Ziel war es, später mit nur einer Zeile (`ask(...)`) Antworten generieren zu können.  Dabei lag der Fokus besonders auf dem Einbau von Kontrollmechanismen, um sicherzustellen,  dass das Tokenbudget nicht überschritten wird.  \n",
        "\n",
        "\n",
        "Im Folgenden wird der Code etwas detaillierter erklärt:\n",
        "\n",
        "- **Parameter**:  \n",
        "  - `question`: Nutzerfrage  \n",
        "  - `embeddings`, `chunks_in`: vorbereitete Datenbasis aus Embeddings + Textabschnitten  \n",
        "  - `k`: wie viele Top-Chunks beim Retrieval genutzt werden  \n",
        "  - `max_ctx_chars`: begrenzt die Länge des eingebauten Kontexts  \n",
        "  - `max_answer_tokens`: maximale Länge der Modell-Antwort  \n",
        "\n",
        "- **Ablauf**:  \n",
        "  1. Die Nutzerfrage wird eingebettet und mit allen Chunks verglichen → Top-k relevante Chunks.  \n",
        "  2. Der Kontext wird aus diesen Chunks zusammengesetzt, aber auf `max_ctx_chars` Zeichen begrenzt.  \n",
        "  3. Prompt wird erstellt:  \n",
        "     - *System-Message*: definiert Regeln („antworte nur aus dem Kontext“).  \n",
        "     - *User-Message*: enthält Kontext + Nutzerfrage.  \n",
        "  4. Mit `client.chat.completions.create` wird das LLM aufgerufen.  \n",
        "  5. Die erste Modellantwort wird extrahiert und als String zurückgegeben.  \n",
        "\n",
        "Damit ist die komplette RAG-Pipeline abgeschlossen: **eine Nutzerfrage genügt, und das System führt Retrieval, Kontextbau und Antwortgenerierung automatisch aus.*\n"
      ],
      "metadata": {
        "id": "C3iU2QrVe_ov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline-Funktion: Retrieval → LLM\n",
        "def ask(question: str,\n",
        "        embeddings: np.ndarray = embeddings_transcript,\n",
        "        chunks_in: list[str] = chunks,\n",
        "        k: int = top_k,\n",
        "        max_context_chars: int = 3000,\n",
        "        max_answer_tokens: int = 300):\n",
        "\n",
        "    # Top-k Chunks holen\n",
        "    retrieved = retrieve_top_k(question, embeddings, chunks_in, k)\n",
        "\n",
        "    # Kontext zusammenbauen (begrenzt auf max_context_chars)\n",
        "    context_parts, used = [], 0\n",
        "    for idx, score, txt in retrieved:\n",
        "        if used + len(txt) > max_context_chars:\n",
        "            txt = txt[: max_context_chars - used]\n",
        "        context_parts.append(txt.strip())\n",
        "        used += len(txt)\n",
        "        if used >= max_context_chars:\n",
        "            break\n",
        "    context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
        "\n",
        "    # Prompt definieren\n",
        "    system_msg = (\n",
        "        \"You are a concise assistant. Answer ONLY using the provided context. \"\n",
        "        \"If the answer is not in the context, say you don't know.\"\n",
        "    )\n",
        "    user_msg = f\"Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer in 3–6 sentences.\"\n",
        "\n",
        "    # OpenAI-Call\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model_chat,\n",
        "        temperature=temperature,\n",
        "        max_tokens=min(max_answer_tokens, token_limit),\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_msg},\n",
        "            {\"role\": \"user\", \"content\": user_msg},\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "_kjBa1_GbTvN"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Beispielfragen\n",
        "\n",
        "\n",
        "#answer = ask(\"Fasse die Entwicklung des Tennissports von der Pre-Open Era bis heute in 3–4 Sätzen zusammen.\")\n",
        "#answer = ask(\"Welche Spielerinnen prägten das moderne Frauentennis und wie unterschieden sie sich?\")\n",
        "#answer = ask(\"Welche jungen Spieler werden als die Zukunft des Tennissports genannt und warum?\")\n",
        "answer = ask(\"Welche Spieler gehören zu den Big Three und wodurch zeichnen sie sich jeweils aus?\")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QS0kTBKokE5P",
        "outputId": "1ff8d239-e605-458f-80b5-7638ef1719f5"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Die Spieler, die zu den Big Three gehören, sind Roger Federer, Rafael Nadal und Novak Djokovic. Roger Federer zeichnet sich durch seinen butterweichen Spielstil und die schönste einhändige Rückhand aus; er gewann 20 Grand Slams und dominierte besonders auf Rasen. Rafael Nadal, der König des Sandplatzes, ist bekannt für seine unermüdliche Energie und seinen Kampfgeist, was ihm 14 French Open Titel einbrachte. Novak Djokovic ist für seine außergewöhnliche mentale Stärke bekannt und hat zahlreiche Rekorde gebrochen, darunter die meisten Wochen als Nummer eins der Welt. Gemeinsam haben sie über 60 Grand Slam Titel gewonnen und zahlreiche unvergessliche Duelle ausgetragen.\n"
          ]
        }
      ]
    }
  ]
}